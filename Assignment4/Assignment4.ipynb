{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDP's)\n",
    "\n",
    "### Assignment 4\n",
    "\n",
    "#### Author: Bryan Baysinger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time\n",
    "import itertools\n",
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gym.envs.registration import register\n",
    "from gym import wrappers\n",
    "from hiive.mdptoolbox import mdp\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 - Grid World - Frozen Lake\n",
    "\n",
    "#### Source https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, -1 for falling in a hole, and a small negative reward otherwise.\n",
    "    The hole and step rewards are configurable when creating an instance of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 8  # Set size here\n",
    "frozenLake = generate_random_map(size=size, p=0.8)\n",
    "env = gym.make(\"FrozenLake-v0\", desc=frozenLake, is_slippery=True)\n",
    "env.reset\n",
    "print('Frozen Lake - {} x {}'.format(size, size))\n",
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.P[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup to Play Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "        wins = 0\n",
    "        total_reward = 0\n",
    "        for episode in range(n_episodes):\n",
    "                terminated = False\n",
    "                state = environment.reset()\n",
    "                while not terminated:\n",
    "                        # Select best action to perform in a current state\n",
    "                        action = np.argmax(policy[state])\n",
    "                        # Perform an action an observe how environment acted in response\n",
    "                        next_state, reward, terminated, info = environment.step(action)\n",
    "                        # Summarize total reward\n",
    "                        total_reward += reward\n",
    "                        # Update current state\n",
    "                        state = next_state\n",
    "                        # Calculate number of wins over episodes\n",
    "                        if terminated and reward == 1.0:\n",
    "                                wins += 1\n",
    "        average_reward = total_reward / n_episodes\n",
    "        return wins, total_reward, average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward(env):\n",
    "    n_states, n_actions = env.nS, env.nA\n",
    "    \n",
    "    R = np.zeros((n_states, n_actions))\n",
    "    for s in range(n_states):\n",
    "        for a, moves in env.env.P[s].items():\n",
    "            for possible_move in moves:\n",
    "                prob, _, r, _ = possible_move\n",
    "                R[s, a] += r * prob\n",
    "    \n",
    "    return R\n",
    "\n",
    "def getProb(env):\n",
    "    n_states, n_actions = env.nS, env.nA\n",
    "    \n",
    "    P = np.zeros((n_states, n_actions, n_states))\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            for moves in env.env.P[s][a]:\n",
    "                prob, next_s, _, _ = moves\n",
    "                P[s, a, next_s] += prob\n",
    "    \n",
    "    return P\n",
    "\n",
    "def print_value(V, width=size, height=size):\n",
    "    return np.around(np.resize(V, (width, height)), 4)\n",
    "\n",
    "\n",
    "def print_policy(V, width=size, height=size):\n",
    "    table = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "    policy = np.resize(V, (width, height))\n",
    "    \n",
    "    # transform using the dictionary\n",
    "    return np.vectorize(table.get)(policy)\n",
    "\n",
    "def policy_matrix(Q):\n",
    "    table = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "    best_actions = np.argmax(Q, axis=1)\n",
    "    policy = np.resize(best_actions, (size, size))\n",
    "    \n",
    "    # transform using the dictionary\n",
    "    return np.vectorize(table.get)(policy)\n",
    "\n",
    "def plot_values(V, iteration_name):\n",
    "    # reshape value function\n",
    "    V_sq = np.reshape(V, (size,size))\n",
    "    # plot the state-value function\n",
    "    fig = plt.figure(figsize=(size, size))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(V_sq, cmap='cool')\n",
    "    for (j,i),label in np.ndenumerate(V_sq):\n",
    "        ax.text(i, j, np.round(label, 3), ha='center', va='center', fontsize=14)\n",
    "    plt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
    "    plt.title(f'{iteration_name} State-Value Function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variable Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value and Policy\n",
    "theta = 1e-9 # convergence threshold for policy/value iteration\n",
    "discount_factor = 0.95 # discount parameter for past policy/value iterations\n",
    "max_iterations = 1000 # maximum iterations for slowly converging policy/value iteration \n",
    "\n",
    "# Qlearning\n",
    "qepsilon = 0.1 # epsilon value for the Q-learning epsilon greedy strategy\n",
    "lr = 0.8 # Q-learning rate\n",
    "qgamma = 0.95 # Q-Learning discount factor\n",
    "episodes = 10000 # number of Q-learning episodes\n",
    "initial = 0 # value to initialize the Q grid\n",
    "decay = True # Decay qepsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "#### Source: https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellman\n",
    "def value_iteration(environment, discount_factor=discount_factor, theta=theta, max_iterations=max_iterations):\n",
    "        # Initialize state-value function with zeros for each environment state\n",
    "        V = np.zeros(environment.nS)\n",
    "        for i in range(int(max_iterations)):\n",
    "                # Early stopping condition\n",
    "                delta = 0\n",
    "                # Update each state\n",
    "                for state in range(environment.nS):\n",
    "                        # Do a one-step lookahead to calculate state-action values\n",
    "                        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                        # Select best action to perform based on the highest state-action value\n",
    "                        best_action_value = np.max(action_value)\n",
    "                        # Calculate change in value\n",
    "                        delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "                        # Update the value function for current state\n",
    "                        V[state] = best_action_value\n",
    "                        # Check if we can stop\n",
    "                if delta < theta:\n",
    "                        print(f'Value-iteration converged at iteration # {i}.')\n",
    "                        break\n",
    "\n",
    "        # Create a deterministic policy using the optimal value function\n",
    "        policy = np.zeros([environment.nS, environment.nA])\n",
    "        for state in range(environment.nS):\n",
    "                # One step lookahead to find the best action for this state\n",
    "                action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                # Select best action based on the highest state-action value\n",
    "                best_action = np.argmax(action_value)\n",
    "                # Update the policy to perform a better action at a current state\n",
    "                policy[state, best_action] = 1.0\n",
    "        return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "#### Source: https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=discount_factor, theta=theta, max_iterations=max_iterations):\n",
    "        # Number of evaluation iterations\n",
    "        evaluation_iterations = 1\n",
    "        # Initialize a value function for each state as zero\n",
    "        V = np.zeros(environment.nS)\n",
    "        # Repeat until change in value is below the threshold\n",
    "        for i in range(int(max_iterations)):\n",
    "                # Initialize a change of value function as zero\n",
    "                delta = 0\n",
    "                # Iterate though each state\n",
    "                for state in range(environment.nS):\n",
    "                       # Initial a new value of current state\n",
    "                       v = 0\n",
    "                       # Try all possible actions which can be taken from this state\n",
    "                       for action, action_probability in enumerate(policy[state]):\n",
    "                             # Check how good next state will be\n",
    "                             for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                                  # Calculate the expected value\n",
    "                                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "                       \n",
    "                       # Calculate the absolute change of value function\n",
    "                       delta = max(delta, np.abs(V[state] - v))\n",
    "                       # Update value function\n",
    "                       V[state] = v\n",
    "                evaluation_iterations += 1\n",
    "                \n",
    "                # Terminate if value change is insignificant\n",
    "                if delta < theta:\n",
    "                        print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "                        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "        action_values = np.zeros(environment.nA)\n",
    "        for action in range(environment.nA):\n",
    "                for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                        action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=discount_factor, max_iterations=max_iterations):\n",
    "        # Start with a random policy\n",
    "        #num states x num actions / num actions\n",
    "        policy = np.ones([environment.nS, environment.nA]) / environment.nA\n",
    "        # Initialize counter of evaluated policies\n",
    "        evaluated_policies = 1\n",
    "        # Repeat until convergence or critical number of iterations reached\n",
    "        for i in range(int(max_iterations)):\n",
    "                stable_policy = True\n",
    "                # Evaluate current policy\n",
    "                V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "                # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "                for state in range(environment.nS):\n",
    "                        # Choose the best action in a current state under current policy\n",
    "                        current_action = np.argmax(policy[state])\n",
    "                        # Look one step ahead and evaluate if current action is optimal\n",
    "                        # We will try every possible action in a current state\n",
    "                        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                        # Select a better action\n",
    "                        best_action = np.argmax(action_value)\n",
    "                        # If action didn't change\n",
    "                        if current_action != best_action:\n",
    "                                stable_policy = True\n",
    "                                # Greedy policy update\n",
    "                                policy[state] = np.eye(environment.nA)[best_action]\n",
    "                evaluated_policies += 1\n",
    "                # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "                if stable_policy:\n",
    "                        print(f'Evaluated {evaluated_policies} policies.')\n",
    "                        return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DF to store items for comparison\n",
    "# Also resets\n",
    "df = pd.DataFrame({'Policy':[], \n",
    "                   'Wins':[], \n",
    "                   'Total Reward':[], \n",
    "                   'Avg Reward':[], \n",
    "                   'Discount Factor':[], \n",
    "                   'Num Episodes':[],\n",
    "                   'Time':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = max_iterations\n",
    "env.reset()\n",
    "# Functions to find best policy\n",
    "solvers = [('Policy Iteration', policy_iteration),\n",
    "           ('Value Iteration', value_iteration)]\n",
    "\n",
    "# Set for however many runs you want to average over\n",
    "for i in range(5):\n",
    "    for iteration_name, iteration_func in solvers:\n",
    "        start = time()\n",
    "        # Search for an optimal policy using policy iteration\n",
    "        policy, V = iteration_func(env)\n",
    "        # Apply best policy to the real environment\n",
    "        #start = time()\n",
    "        wins, total_reward, average_reward = play_episodes(env, n_episodes, policy)\n",
    "        stop = time()\n",
    "        \n",
    "        walltime = (stop - start)\n",
    "        \n",
    "        new_row = {'Policy': iteration_name, \n",
    "                   'Wins':wins, \n",
    "                   'Total Reward':total_reward, \n",
    "                   'Avg Reward':average_reward, \n",
    "                   'Discount Factor':discount_factor, \n",
    "                   'Num Episodes':n_episodes,\n",
    "                   'Time':walltime}\n",
    "\n",
    "        df = df.append(new_row, ignore_index=True)\n",
    "        print('-----')\n",
    "        print(\"discount factor: \", discount_factor)\n",
    "        print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "        print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')\n",
    "        print(print_policy(policy))\n",
    "        plot_values(V, iteration_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Policy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"output.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Reinforcement Learning - Package to Experiment with Results from Above\n",
    "\n",
    "#### Source: https://lrl.readthedocs.io/en/latest/example_solution_frozen_lake.html\n",
    "\n",
    "#### Piazza @668 Daniel Boros 1 day ago The point is that you don't use code that automates the exploration for you. See the plagiarism FAQ. If you've done the exploration yourself and are now in the mode of plotting results and generating visuals to aid your analysis, I think that's fine to leverage some of those plot functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lake = environments.frozen_lake.RewardingFrozenLakeEnv(map_name='8x8', is_slippery=True)\n",
    "# Use our env above\n",
    "from lrl import environments, solvers\n",
    "from lrl.utils import plotting\n",
    "lake_vi = solvers.ValueIteration(env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_vi.iterate_to_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data = lake_vi.score_policy(iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'type(scoring_data) = {type(scoring_data)}')\n",
    "scoring_data_df = scoring_data.to_dataframe(include_episodes=True)\n",
    "scoring_data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_data_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_results = plotting.plot_solver_results(env=env, solver=lake_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lake_vi.value.get_value_history(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_pi = solvers.PolicyIteration(env=env)\n",
    "lake_pi.iterate_to_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (these are simple convenience functions for plotting, basically just recipes.  See the plotting API)\n",
    "# We can pass the solver..\n",
    "ax = plotting.plot_solver_convergence(lake_vi, label='vi')\n",
    "\n",
    "# Or going a little deeper into the API, with style being passed to matplotlib's plot function...\n",
    "ax = plotting.plot_solver_convergence_from_df(lake_pi.iteration_data.to_dataframe(), y='delta_max', x='iteration', ax=ax, label='pi', ls='', marker='o')\n",
    "plt.title('Convergence - Change in Value Function for {} x {} Frozen Lake'.format(size, size))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (these are simple convenience functions for plotting, basically just recipes.  See the plotting API)\n",
    "# We can pass the solver..\n",
    "ax = plotting.plot_solver_convergence(lake_vi, y='policy_changes', label='vi')\n",
    "\n",
    "# Or going a little deeper into the API...\n",
    "ax = plotting.plot_solver_convergence_from_df(lake_pi.iteration_data.to_dataframe(), y='policy_changes', x='iteration', ax=ax, label='pi', ls='', marker='o')\n",
    "plt.title('Convergence - Policy Changes for {} x {} Frozen Lake'.format(size, size))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (these are simple convenience functions for plotting, basically just recipes.  See the plotting API)\n",
    "# We can pass the solver..\n",
    "ax = plotting.plot_solver_convergence(lake_vi, y='time', label='vi')\n",
    "\n",
    "# Or going a little deeper into the API...\n",
    "ax = plotting.plot_solver_convergence_from_df(lake_pi.iteration_data.to_dataframe(), y='time', x='iteration', ax=ax, label='pi', ls='', marker='o')\n",
    "plt.title('Convergence Time for {} x {} Frozen Lake'.format(size, size))\n",
    "ax.legend()\n",
    "\n",
    "print(f'Total solution time for Value Iteration (excludes any scoring time):  {lake_vi.iteration_data.to_dataframe().loc[:, \"time\"].sum():.2f}s')\n",
    "print(f'Total solution time for Policy Iteration (excludes any scoring time): {lake_pi.iteration_data.to_dataframe().loc[:, \"time\"].sum():.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "#### Source: https://twice22.github.io/rl-part2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/\n",
    "## Decaying epsilon\n",
    "\n",
    "import math\n",
    "\n",
    "#Episodic\n",
    "def decay1(e):\n",
    "    decay = 1 / float(e + 1)\n",
    "    return decay\n",
    "#Log\n",
    "def decay2(e):\n",
    "    decay = float(math.e**(-e * 0.001))\n",
    "    return decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha=0.80, decay=decay2, gamma=0.95, episodes=10000):\n",
    "    \"\"\" Runs Q-Learning on a gym problem.\n",
    "    Args:\n",
    "        env (gym.env): Gym problem object.\n",
    "        decay (function): Decay function for random action rate.\n",
    "        alpha (float): Learning rate.\n",
    "        gamma (float): Discount rate.\n",
    "        episodes (int): Number of episodes.\n",
    "    Returns:\n",
    "        policy (numpy.array): Optimal policy.\n",
    "        i + 1 (int): Number of iterations until convergence.\n",
    "    \"\"\"\n",
    "    # Q, maximum steps, visits and rewards\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    rewards = []\n",
    "    iterations = []\n",
    "    max_steps = 500\n",
    "    visits = np.zeros((env.observation_space.n, 1))\n",
    "\n",
    "    # episodes\n",
    "    for episode in range(episodes):\n",
    "        # refresh state\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        t_reward = 0\n",
    "\n",
    "        # run episode\n",
    "        for i in range(max_steps):\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            current = state\n",
    "            action = np.argmax(Q[current, :] +\n",
    "                               np.random.randn(1, env.action_space.n) * decay2(episode))\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            visits[state] += 1\n",
    "            t_reward += reward\n",
    "            Q[current, action] += alpha * \\\n",
    "                (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "\n",
    "        rewards.append(t_reward)\n",
    "        iterations.append(i)\n",
    "\n",
    "    return Q, iterations, rewards, visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, s, qepsilon):\n",
    "    p = np.random.uniform()\n",
    "    if p < qepsilon:\n",
    "        # the sample() method from the environment allows\n",
    "        # to randomly sample an action from the set of actions\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # act greedily by selecting the best action possible in the current state\n",
    "        return np.argmax(Q[s, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlearning(env, qepsilon, lr, qgamma, episodes):\n",
    "    # initialize our Q-table: matrix of size [n_states, n_actions] with zeros\n",
    "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    \n",
    "    iterations = []\n",
    "    rewards = []\n",
    "    visits = np.zeros((env.observation_space.n, 1))\n",
    "    \n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        terminate = False # did the game end ?\n",
    "        \n",
    "        totalR = 0 # Reward total\n",
    "        i = 0\n",
    "        \n",
    "        while True:\n",
    "            # choose an action using the epsilon greedy strategy\n",
    "            action = epsilon_greedy(Q, state, qepsilon)\n",
    "\n",
    "            # execute the action. The environment provides us\n",
    "            # 4 values: \n",
    "            # - the next_state we ended in after executing our action\n",
    "            # - the reward we get from executing that action\n",
    "            # - wether or not the game ended\n",
    "            # - the probability of executing our action \n",
    "            # (we don't use this information here)\n",
    "            next_state, reward, terminate, _ = env.step(action)\n",
    "\n",
    "            if reward == 0: # if we didn't reach the goal state\n",
    "                if terminate: # if the agent falls in an hole\n",
    "                    r = -50 # then give them a big negative reward\n",
    "                    totalR += r\n",
    "\n",
    "                    # the Q-value of the terminal state equals the reward\n",
    "                    Q[next_state] = np.ones(n_actions) * r\n",
    "                    \n",
    "                else: # the agent is in a frozen tile\n",
    "                    r = -1 # give the agent a little negative reward to avoid long episode\n",
    "                    totalR += r\n",
    "            \n",
    "            if reward == 1: # the agent reach the goal state\n",
    "                r = 100 # give him a big reward\n",
    "                totalR += r\n",
    "\n",
    "                # the Q-value of the terminal state equals the reward\n",
    "                Q[next_state] = np.ones(n_actions) * r\n",
    "\n",
    "            visits[state] += 1\n",
    "\n",
    "            \n",
    "            # Q-learning update\n",
    "            Q[state,action] = Q[state,action] + lr * (r + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "            # move the agent to the new state before executing the next iteration\n",
    "            state = next_state\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "            # if we reach the goal state or fall in an hole\n",
    "            # end the current episode\n",
    "            if terminate:\n",
    "                break\n",
    "                \n",
    "        rewards.append(totalR)\n",
    "        iterations.append(i)\n",
    "            \n",
    "    return Q, iterations, rewards, visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "qepsilon = 0.1 # epsilon value for the epsilon greedy strategy\n",
    "lr = 0.8 # learning rate\n",
    "qgamma = 0.95 # discount factor\n",
    "episodes = 10000 # number of episode\n",
    "\n",
    "#Q, iterations, rewards, visits = Qlearning(env, qepsilon, lr, qgamma, episodes)\n",
    "Q, iterations, rewards, visits = q_learning(env, alpha=0.50, decay=decay2, gamma=0.95, episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    rewards_smoothed = pd.Series(rewards).rolling(100, min_periods=100).mean()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    iterations_smoothed = pd.Series(iterations).rolling(100, min_periods=100).mean()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(iterations_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Iterations (Smoothed)\")\n",
    "    plt.title(\"Episode Iterations over Time (Smoothed over window size {})\".format(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQ = pd.DataFrame({'Iterations': iterations,\n",
    "                   'Rewards': rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQ.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "\n",
    "    # plot reward curve\n",
    "    r = dfQ['Rewards']\n",
    "    ax.plot(r, color='b')\n",
    "    ax.set_title('Average Rewards ({})'.format('Q Learning'))\n",
    "    ax.set_ylabel('Average Reward')\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.grid(linestyle='dotted')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlearning_trajectory(env, Q, max_steps=500):\n",
    "    state = env.reset() # reinitialize the environment\n",
    "    i = 0\n",
    "    while i < max_steps:\n",
    "        # once the agent has been trained, it\n",
    "        # will take the best action in each state\n",
    "        action = np.argmax(Q[state,:])\n",
    "\n",
    "        # execute the action and recover a tuple of values\n",
    "        next_state, reward, terminate, _ = env.step(action)\n",
    "        print(\"####################\")\n",
    "        env.render() # display the new state of the game\n",
    "\n",
    "        # move the agent to the new state before executing the next iteration\n",
    "        state = next_state\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "        # if the agent falls in a hole or ends in the goal state\n",
    "        if terminate:\n",
    "            break # break out of the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qlearning_trajectory(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_matrix(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Look at Q Learning\n",
    "\n",
    "#### Source https://github.com/theone9807/8x8-FrozenLake-Q-Learning/blob/master/8x8%20frozenlake(task%201).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Q-table\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide all initial values\n",
    "\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode =  400\n",
    "\n",
    "learning_rate = 0.5    # notation - η or α\n",
    "discount_rate = 0.95    #notation - γ(gamma)\n",
    "\n",
    "exploration_rate = 1   #notation - ε\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.1\n",
    "exploration_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This algorithm is for reward and step updation\n",
    "\n",
    "rewards_all_episodes = []  \n",
    "episode_steps = []\n",
    "\n",
    "#Q - Learning algo\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        #Exploration - exploitation trade- off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)   \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)  #tuple unpacking\n",
    "        \n",
    "        \n",
    "        #Updating Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        \n",
    "        state = new_state               # change state to new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    #Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    episode_steps.append(step)     # this is important step \n",
    "        \n",
    "#calculating & printing the average reward per thousand episodes\n",
    " \n",
    "rewards_per_50_episodes =np.split(np.array(rewards_all_episodes), num_episodes/50)    \n",
    "count = 50\n",
    "print(\"********Average rewards per 50 episodes********\\n\")\n",
    "for r in rewards_per_50_episodes:\n",
    "    print(count, \": \", str(sum(r/50)))                                                  \n",
    "    count += 50\n",
    "            \n",
    "#print updated Q-table\n",
    "print(\"\\n\\n*******Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watching the agent play\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Change range to watch\n",
    "for episode in range(0):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"*****Episode \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You have reached the goal!****\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(2)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        step += 1\n",
    "   # state = new_state\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the success rate for solving the environment & elapsed training time\n",
    "success_rate = round((sum(rewards_all_episodes) / num_episodes) * 100, 2)\n",
    "#elapsed_training_time = int(training_end - training_start)\n",
    "print(\"\\nThis environment has been solved\", str(success_rate), \"% of times over\",  str(num_episodes), \"episodes.\")\n",
    "\n",
    "# plot the rewards and number of steps over all training episodes\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(rewards_all_episodes, '-g', label = 'reward')\n",
    "ax1.set_yticks([0,1])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(episode_steps, '+r', label = 'step')\n",
    "ax1.set_xlabel(\"episode\")\n",
    "ax1.set_ylabel(\"reward\")\n",
    "ax2.set_ylabel(\"step\")\n",
    "ax1.legend(loc=2)\n",
    "ax2.legend(loc=1)\n",
    "plt.title(\"Training Stats\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet another Look at Q Learning Frozen Lake\n",
    "\n",
    "#### Source: https://lrl.readthedocs.io/en/latest/example_solution_frozen_lake.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need deterministic for good results\n",
    "\n",
    "# Let's be explicit with our QLearning settings for alpha and epsilon\n",
    "alpha = 0.9  # Constant alpha during learning\n",
    "\n",
    "# Decay function for epsilon (see QLearning() and decay_functions() in documentation for syntax)\n",
    "# Decay epsilon linearly from 0.2 at timestep (iteration) 0 to 0.05 at timestep 1500,\n",
    "# keeping constant at 0.05 for ts>1500\n",
    "epsilon = {\n",
    "    'type': 'linear',\n",
    "    'initial_value': 0.2,\n",
    "    'initial_timestep': 0,\n",
    "    'final_value': 0.05,\n",
    "    'final_timestep': 1500\n",
    "}\n",
    "\n",
    "# Above PI/VI used the default gamma, but we will specify one here\n",
    "gamma = 0.95\n",
    "\n",
    "# Convergence is kinda tough to interpret automatically for Q-Learning.  One good way to monitor convergence is to\n",
    "# evaluate how good the greedy policy at a given point in the solution is and decide if it is still improving.\n",
    "# We can enable this with score_while_training (available for Value and Policy Iteration as well)\n",
    "# NOTE: During scoring runs, the solver is acting greedily and NOT learning from the environment.  These are separate\n",
    "#       runs solely used to estimate solution progress\n",
    "# NOTE: Scoring every 50 iterations is probably a bit much, but used to show a nice plot below.  The default 500/500\n",
    "#       is probably a better general guidance\n",
    "score_while_training = {\n",
    "    'n_trains_per_eval': 100,  # Number of training episodes we run per attempt to score the greedy policy\n",
    "                               # (eg: Here we do a scoring run after every 500 training episodes, where training episodes\n",
    "                               # are the usual epsilon-greedy exploration episodes)\n",
    "    'n_evals': 250,  # Number of times we run through the env with the greedy policy whenever we score\n",
    "}\n",
    "# score_while_training = True  # This calls the default settings, which are also 500/500 like above\n",
    "\n",
    "lake_ql = solvers.QLearning(env=env, alpha=alpha, epsilon=epsilon, gamma=gamma,\n",
    "                          max_iters=10000, score_while_training=score_while_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_ql.iterate_to_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_ql_iter_df = lake_ql.iteration_data.to_dataframe()\n",
    "lake_ql_iter_df.plot(x='iteration', y='policy_changes', kind='scatter', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_ql_intermediate_scoring_df = lake_ql.scoring_summary.to_dataframe()\n",
    "lake_ql_intermediate_scoring_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lake_ql_intermediate_scoring_df.loc[:, 'iteration'], lake_ql_intermediate_scoring_df.loc[:, 'reward_mean'], '-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 - Optimal Fire Management\n",
    "\n",
    "#### Source: http://sawcordwell.github.io/mdp/conservation/2015/01/10/possingham1997-1/\n",
    "\n",
    "#### Paper: http://www.mssanz.org.au/MODSIM97/Vol%202/Possingham.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire Management Environment\n",
    "\n",
    "Source: Andrew Rollings https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjzh7b4sw45n6rv%2Fidfghtz88na6xb%2Fk30cobhdj4jp%2Ffire_management_spec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values_fire(V, iteration_name):\n",
    "    # reshape value function\n",
    "    V_sq = np.reshape(V, (fm_spec.population_classes,fm_spec.fire_classes))\n",
    "    # plot the state-value function\n",
    "    fig = plt.figure(figsize=(fm_spec.population_classes,fm_spec.fire_classes))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(V_sq, cmap='cool')\n",
    "    for (j,i),label in np.ndenumerate(V_sq):\n",
    "        ax.text(i, j, np.round(label, 3), ha='center', va='center', fontsize=14)\n",
    "    plt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
    "    plt.title(f'{iteration_name} State-Value Function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load fire_management_spec.py\n",
    "import numpy as np\n",
    "from hiive.visualization import mdpviz\n",
    "\n",
    "# Two tests here 4 x 8 and 8 x 24\n",
    "\n",
    "class FireManagementSpec:\n",
    "\n",
    "    def __init__(self, population_classes=7, fire_classes=13, seed=1, verbose=True):\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.population_classes = population_classes\n",
    "        self.fire_classes = fire_classes\n",
    "        self.states = {}\n",
    "\n",
    "        self.spec = mdpviz.MDPSpec()\n",
    "\n",
    "        self._action_do_nothing = self.spec.action('do_nothing')\n",
    "        self._action_burn = self.spec.action('burn')\n",
    "\n",
    "        self._probabilities = {}\n",
    "        self.name = f'fire_management_{population_classes}_{fire_classes}_{seed}'\n",
    "        self.n_actions = 2\n",
    "        self.n_states = self.fire_classes * self.population_classes\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        np.random.seed(self.seed)\n",
    "        self._setup_mdp()\n",
    "\n",
    "    def _reset_state_probabilities(self):\n",
    "        self._probabilities = {}\n",
    "\n",
    "    def _get_probability_for_state(self, pc, fc):\n",
    "        state_name = self._get_state_name(pc, fc)\n",
    "        if state_name not in self._probabilities:\n",
    "            return None\n",
    "        return self._probabilities[state_name]\n",
    "\n",
    "    def _set_probability_for_state(self, pc, fc, p):\n",
    "        state_name = self._get_state_name(pc, fc)\n",
    "        if state_name not in self._probabilities:\n",
    "            self._probabilities[state_name] = 0.\n",
    "        self._probabilities[state_name] += p\n",
    "        return self._probabilities[state_name]\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_terminal(s):\n",
    "        return False  # s == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_habitat_suitability(years):\n",
    "        if years < 0:\n",
    "            msg = \"Invalid years '%s', it should be positive.\" % str(years)\n",
    "            raise ValueError(msg)\n",
    "        if years <= 5:\n",
    "            return 0.2 * years\n",
    "        elif 5 <= years <= 10:\n",
    "            return -0.1 * years + 1.5\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_state_name(pc, fc):\n",
    "        return f'pc:{pc}, fc:{fc}'\n",
    "\n",
    "    def _get_state(self, pc, fc):\n",
    "        state_name = self._get_state_name(pc, fc)\n",
    "        is_terminal = self._is_terminal(pc)\n",
    "        if state_name not in self.states:\n",
    "            state = self.spec.state(name=state_name, terminal_state=is_terminal)\n",
    "            self.states[state_name] = state\n",
    "        # print(f'{state_name} : {is_terminal}')\n",
    "        state = self.states[state_name]\n",
    "        return state\n",
    "\n",
    "    def _add_state_transition_and_reward(self, pc, fc, action):\n",
    "        cs = self._get_state(pc, fc)\n",
    "        results = self._get_reward_and_new_state_values(pc, fc, action)\n",
    "        for reward, npc, nfc, tp in results:\n",
    "            ns = self._get_state(npc, nfc)\n",
    "            ns = mdpviz.NextState(state=ns, weight=tp)\n",
    "            self.spec.transition(state=cs, action=action, outcome=ns)\n",
    "            self.spec.transition(state=cs, action=action, outcome=mdpviz.Reward(reward))\n",
    "            if self.verbose:\n",
    "                print(f'[state:action]: [{(pc, fc)} : {action.name}] -> new state: {(npc, nfc)}, '\n",
    "                      f'p(t): {tp}, reward: {reward} ')\n",
    "\n",
    "    def transition_fire_class(self, fc, action):\n",
    "        if action == self._action_do_nothing:\n",
    "            return (fc + 1) if fc < self.fire_classes - 1 else fc\n",
    "        elif action == self._action_burn:\n",
    "            return 0\n",
    "        return fc\n",
    "\n",
    "    def _get_reward_and_new_state_values(self, pc, fc, action, default_p=0.5):\n",
    "        pop_change_down = -1\n",
    "        pop_change_same = 0\n",
    "\n",
    "        self._probabilities = {}\n",
    "        transition_probability_up = None\n",
    "#        if pc == 1 and fc == 0 and action == self._action_burn:\n",
    "#            print()\n",
    "\n",
    "        r = self.get_habitat_suitability(fc)\n",
    "        fc = self.transition_fire_class(fc, action)\n",
    "        if pc == 0:\n",
    "            # dead\n",
    "            return [[0.0, 0, fc, 1.0]]  # stays in same state\n",
    "        if pc == self.population_classes - 1:\n",
    "            pop_change_up = 0\n",
    "            if action == self._action_burn:\n",
    "                pop_change_same -= 1\n",
    "                pop_change_down -= 1\n",
    "\n",
    "            tsd = self._set_probability_for_state(pc=pc + pop_change_down,\n",
    "                                                  fc=fc,\n",
    "                                                  p=(1.0 - default_p) * (1.0 - r))\n",
    "            tss = self._set_probability_for_state(pc=pc + pop_change_same,\n",
    "                                                  fc=fc,\n",
    "                                                  p=1.0 - tsd)\n",
    "        else:\n",
    "            # Population abundance class can stay the same, transition up, or\n",
    "            # transition down.\n",
    "            pop_change_same = 0\n",
    "            pop_change_up = 1\n",
    "            pop_change_down = -1\n",
    "\n",
    "            # If action 1 is taken, then the patch is burned so the population\n",
    "            # abundance moves down a class.\n",
    "            if action == self._action_burn:\n",
    "                pop_change_same -= 1\n",
    "                pop_change_up -= 1\n",
    "                pop_change_down -= (1 if pop_change_down > 0 else 0)\n",
    "\n",
    "            tss = self._set_probability_for_state(pc=pc + pop_change_same,\n",
    "                                                  fc=fc,\n",
    "                                                  p=default_p)\n",
    "\n",
    "            tsu = self._set_probability_for_state(pc=pc + pop_change_up,\n",
    "                                                  fc=fc,\n",
    "                                                  p=(1 - default_p)*r)\n",
    "            # In the case when transition_down = 0 before the effect of an action\n",
    "            # is applied, then the final state is going to be the same as that for\n",
    "            # transition_same, so we need to add the probabilities together.\n",
    "            tsd = self._set_probability_for_state(pc=pc + pop_change_down,\n",
    "                                                  fc=fc,\n",
    "                                                  p=(1 - default_p)*(1 - r))\n",
    "\n",
    "        # build results\n",
    "        results = []\n",
    "\n",
    "        npc_up = pc + pop_change_up\n",
    "        npc_down = pc + pop_change_down\n",
    "        npc_same = pc + pop_change_same\n",
    "\n",
    "        transition_probabilities = {\n",
    "            (npc_up, self._get_probability_for_state(npc_up, fc)),\n",
    "            (npc_down, self._get_probability_for_state(npc_down, fc)),\n",
    "            (npc_same, self._get_probability_for_state(npc_same, fc))\n",
    "        }\n",
    "\n",
    "        for npc, probability in transition_probabilities:\n",
    "            if probability is not None and probability > 0.0:\n",
    "                reward = int(npc > 0)\n",
    "                results.append((reward, npc, fc, probability))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # noinspection PyStatementEffect\n",
    "    def _setup_mdp(self):\n",
    "        # build transitions\n",
    "        for pc in range(0, self.population_classes):\n",
    "            if self._is_terminal(pc):\n",
    "                continue\n",
    "            for fc in range(0, self.fire_classes):\n",
    "                # actions\n",
    "                self._add_state_transition_and_reward(pc=pc, fc=fc, action=self._action_do_nothing)\n",
    "                self._add_state_transition_and_reward(pc=pc, fc=fc, action=self._action_burn)\n",
    "                if self.verbose:\n",
    "                    print()\n",
    "\n",
    "    def get_transition_and_reward_arrays(self, p_default=0.5):\n",
    "        return self.spec.get_transition_and_reward_arrays(p_default)\n",
    "\n",
    "    def to_graph(self):\n",
    "        return self.spec.to_graph()\n",
    "\n",
    "    def to_env(self):\n",
    "        return self.spec.to_discrete_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_spec = FireManagementSpec()\n",
    "envFM = fm_spec.to_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_spec.reset()\n",
    "fm_spec._setup_mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the state space and action space\n",
    "print(envFM.observation_space)\n",
    "print(envFM.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R = fm_spec.get_transition_and_reward_arrays(p_default=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For charting easier than going back to the top of notebook\n",
    "discount_factor = .10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = mdp.PolicyIteration(P, R, discount_factor, policy0=None, max_iter=1000, eval_type=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "statsPI = pi.run()\n",
    "stop = time()\n",
    "totalTime = stop-start\n",
    "print('Time to train: ', totalTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPI = pd.DataFrame(statsPI)\n",
    "dfPI.to_excel('outputFM_PI.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piShape = np.asarray(pi.policy).reshape(fm_spec.population_classes, fm_spec.fire_classes)\n",
    "plot_values_fire(piShape, 'Policy Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = mdp.ValueIteration(P, R, discount_factor, epsilon=0.01, max_iter=1000, initial_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "statsVI = vi.run()\n",
    "stop = time()\n",
    "totalTime = stop-start\n",
    "print('Time to train: ', totalTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVI = pd.DataFrame(statsVI)\n",
    "dfVI.to_excel('outputFM_VI.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viShape = np.asarray(vi.policy).reshape(fm_spec.population_classes, fm_spec.fire_classes)\n",
    "plot_values_fire(viShape, 'Value Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check converge\n",
    "\n",
    "expected = pi.policy\n",
    "all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Fire Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check sizes\n",
    "action_space_size = envFM.action_space.n\n",
    "state_space_size = envFM.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Action space: ', action_space_size)\n",
    "print('State space: ', state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q, iterations, rewards, visits = q_learning(envFM, alpha=0.50, decay=decay2, gamma=0.95, episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = mdp.QLearning(P, R, discount_factor, alpha=0.80, alpha_decay=0.95, alpha_min=0.01,\n",
    "                 epsilon=.10, epsilon_min=.01, epsilon_decay=0.01,\n",
    "                 n_iter=10000, skip_check=False, iter_callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "statsQ = Q.run()\n",
    "stop = time()\n",
    "totalTime = stop-start\n",
    "print('Time to train: ', totalTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QShape = np.asarray(Q.policy).reshape(fm_spec.population_classes, fm_spec.fire_classes)\n",
    "plot_values_fire(QShape, 'Q Learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for graphs\n",
    "dfFMQ = pd.DataFrame(statsQ)\n",
    "dfFMQ.to_excel('outputFM_Q.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Fire Example\n",
    "\n",
    "#### Source: https://github.com/sawcordwell/pymdptoolbox/blob/master/src/examples/firemdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of population abundance classes\n",
    "POPULATION_CLASSES = 7\n",
    "# The number of years since a fire classes\n",
    "FIRE_CLASSES = 13\n",
    "# The number of states\n",
    "STATES = POPULATION_CLASSES * FIRE_CLASSES\n",
    "# The number of actions\n",
    "ACTIONS = 2\n",
    "ACTION_NOTHING = 0\n",
    "ACTION_BURN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_action(x):\n",
    "    \"\"\"Check that the action is in the valid range.\n",
    "    \"\"\"\n",
    "    if not (0 <= x < ACTIONS):\n",
    "        msg = \"Invalid action '%s', it should be in {0, 1}.\" % str(x)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "def check_population_class(x):\n",
    "    \"\"\"Check that the population abundance class is in the valid range.\n",
    "    \"\"\"\n",
    "    if not (0 <= x < POPULATION_CLASSES):\n",
    "        msg = \"Invalid population class '%s', it should be in {0, 1, …, %d}.\" \\\n",
    "              % (str(x), POPULATION_CLASSES - 1)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "def check_fire_class(x):\n",
    "    \"\"\"Check that the time in years since last fire is in the valid range.\n",
    "    \"\"\"\n",
    "    if not (0 <= x < FIRE_CLASSES):\n",
    "        msg = \"Invalid fire class '%s', it should be in {0, 1, …, %d}.\" % \\\n",
    "              (str(x), FIRE_CLASSES - 1)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "def check_probability(x, name=\"probability\"):\n",
    "    \"\"\"Check that a probability is between 0 and 1.\n",
    "    \"\"\"\n",
    "    if not (0 <= x <= 1):\n",
    "        msg = \"Invalid %s '%s', it must be in [0, 1].\" % (name, str(x))\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_habitat_suitability(years):\n",
    "    \"\"\"The habitat suitability of a patch relatve to the time since last fire.\n",
    "    The habitat quality is low immediately after a fire, rises rapidly until\n",
    "    five years after a fire, and declines once the habitat is mature. See\n",
    "    Figure 2 in Possingham and Tuck (1997) for more details.\n",
    "    Parameters\n",
    "    ----------\n",
    "    years : int\n",
    "        The time in years since last fire.\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "        The habitat suitability.\n",
    "    \"\"\"\n",
    "    if years < 0:\n",
    "        msg = \"Invalid years '%s', it should be positive.\" % str(years)\n",
    "        raise ValueError(msg)\n",
    "    if years <= 5:\n",
    "        return 0.2*years\n",
    "    elif 5 <= years <= 10:\n",
    "        return -0.1*years + 1.5\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_index(population, fire):\n",
    "    \"\"\"Convert state parameters to transition probability matrix index.\n",
    "    Parameters\n",
    "    ----------\n",
    "    population : int\n",
    "        The population abundance class of the threatened species.\n",
    "    fire : int\n",
    "        The time in years since last fire.\n",
    "    Returns\n",
    "    -------\n",
    "    index : int\n",
    "        The index into the transition probability matrix that corresponds to\n",
    "        the state parameters.\n",
    "    \"\"\"\n",
    "    check_population_class(population)\n",
    "    check_fire_class(fire)\n",
    "    return population*FIRE_CLASSES + fire\n",
    "\n",
    "\n",
    "def convert_index_to_state(index):\n",
    "    \"\"\"Convert transition probability matrix index to state parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    index : int\n",
    "        The index into the transition probability matrix that corresponds to\n",
    "        the state parameters.\n",
    "    Returns\n",
    "    -------\n",
    "    population, fire : tuple of int\n",
    "        ``population``, the population abundance class of the threatened\n",
    "        species. ``fire``, the time in years since last fire.\n",
    "    \"\"\"\n",
    "    if not (0 <= index < STATES):\n",
    "        msg = \"Invalid index '%s', it should be in {0, 1, …, %d}.\" % \\\n",
    "              (str(index), STATES - 1)\n",
    "        raise ValueError(msg)\n",
    "    population = index // FIRE_CLASSES\n",
    "    fire = index % FIRE_CLASSES\n",
    "    return (population, fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_fire_state(F, a):\n",
    "    \"\"\"Transition the years since last fire based on the action taken.\n",
    "    Parameters\n",
    "    ----------\n",
    "    F : int\n",
    "        The time in years since last fire.\n",
    "    a : int\n",
    "        The action undertaken.\n",
    "    Returns\n",
    "    -------\n",
    "    F : int\n",
    "        The time in years since last fire.\n",
    "    \"\"\"\n",
    "    ## Efect of action on time in years since fire.\n",
    "    if a == ACTION_NOTHING:\n",
    "        # Increase the time since the patch has been burned by one year.\n",
    "        # The years since fire in patch is absorbed into the last class\n",
    "        if F < FIRE_CLASSES - 1:\n",
    "            F += 1\n",
    "    elif a == ACTION_BURN:\n",
    "        # When the patch is burned set the years since fire to 0.\n",
    "        F = 0\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probabilities(s, x, F, a):\n",
    "    \"\"\"Calculate the transition probabilities for the given state and action.\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : float\n",
    "        The class-independent probability of the population staying in its\n",
    "        current population abundance class.\n",
    "    x : int\n",
    "        The population abundance class of the threatened species.\n",
    "    F : int\n",
    "        The time in years since last fire.\n",
    "    a : int\n",
    "        The action undertaken.\n",
    "    Returns\n",
    "    -------\n",
    "    prob : array\n",
    "        The transition probabilities as a vector from state (``x``, ``F``) to\n",
    "        every other state given that action ``a`` is taken.\n",
    "    \"\"\"\n",
    "    # Check that input is in range\n",
    "    check_probability(s)\n",
    "    check_population_class(x)\n",
    "    check_fire_class(F)\n",
    "    check_action(a)\n",
    "\n",
    "    # a vector to store the transition probabilities\n",
    "    prob = np.zeros(STATES)\n",
    "\n",
    "    # the habitat suitability value\n",
    "    r = get_habitat_suitability(F)\n",
    "    F = transition_fire_state(F, a)\n",
    "\n",
    "    ## Population transitions\n",
    "    if x == 0:\n",
    "        # population abundance class stays at 0 (extinct)\n",
    "        new_state = convert_state_to_index(0, F)\n",
    "        prob[new_state] = 1\n",
    "    elif x == POPULATION_CLASSES - 1:\n",
    "        # Population abundance class either stays at maximum or transitions\n",
    "        # down\n",
    "        transition_same = x\n",
    "        transition_down = x - 1\n",
    "        # If action 1 is taken, then the patch is burned so the population\n",
    "        # abundance moves down a class.\n",
    "        if a == ACTION_BURN:\n",
    "            transition_same -= 1\n",
    "            transition_down -= 1\n",
    "        # transition probability that abundance stays the same\n",
    "        new_state = convert_state_to_index(transition_same, F)\n",
    "        prob[new_state] = 1 - (1 - s)*(1 - r)\n",
    "        # transition probability that abundance goes down\n",
    "        new_state = convert_state_to_index(transition_down, F)\n",
    "        prob[new_state] = (1 - s)*(1 - r)\n",
    "    else:\n",
    "        # Population abundance class can stay the same, transition up, or\n",
    "        # transition down.\n",
    "        transition_same = x\n",
    "        transition_up = x + 1\n",
    "        transition_down = x - 1\n",
    "        # If action 1 is taken, then the patch is burned so the population\n",
    "        # abundance moves down a class.\n",
    "        if a == ACTION_BURN:\n",
    "            transition_same -= 1\n",
    "            transition_up -= 1\n",
    "            # Ensure that the abundance class doesn't go to -1\n",
    "            if transition_down > 0:\n",
    "                transition_down -= 1\n",
    "        # transition probability that abundance stays the same\n",
    "        new_state = convert_state_to_index(transition_same, F)\n",
    "        prob[new_state] = s\n",
    "        # transition probability that abundance goes up\n",
    "        new_state = convert_state_to_index(transition_up, F)\n",
    "        prob[new_state] = (1 - s)*r\n",
    "        # transition probability that abundance goes down\n",
    "        new_state = convert_state_to_index(transition_down, F)\n",
    "        # In the case when transition_down = 0 before the effect of an action\n",
    "        # is applied, then the final state is going to be the same as that for\n",
    "        # transition_same, so we need to add the probabilities together.\n",
    "        prob[new_state] += (1 - s)*(1 - r)\n",
    "\n",
    "    # Make sure that the probabilities sum to one\n",
    "    assert (prob.sum() - 1) < np.spacing(1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_and_reward_arrays(s):\n",
    "    \"\"\"Generate the fire management transition and reward matrices.\n",
    "    The output arrays from this function are valid input to the mdptoolbox.mdp\n",
    "    classes.\n",
    "    Let ``S`` = number of states, and ``A`` = number of actions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : float\n",
    "        The class-independent probability of the population staying in its\n",
    "        current population abundance class.\n",
    "    Returns\n",
    "    -------\n",
    "    out : tuple\n",
    "        ``out[0]`` contains the transition probability matrices P and\n",
    "        ``out[1]`` contains the reward vector R. P is an  ``A`` × ``S`` × ``S``\n",
    "        numpy array and R is a numpy vector of length ``S``.\n",
    "    \"\"\"\n",
    "    check_probability(s)\n",
    "\n",
    "    # The transition probability array\n",
    "    transition = np.zeros((ACTIONS, STATES, STATES))\n",
    "    # The reward vector\n",
    "    reward = np.zeros(STATES)\n",
    "    # Loop over all states\n",
    "    for idx in range(STATES):\n",
    "        # Get the state index as inputs to our functions\n",
    "        x, F = convert_index_to_state(idx)\n",
    "        # The reward for being in this state is 1 if the population is extant\n",
    "        if x != 0:\n",
    "            reward[idx] = 1\n",
    "        # Loop over all actions\n",
    "        for a in range(ACTIONS):\n",
    "            # Assign the transition probabilities for this state, action pair\n",
    "            transition[a][idx] = get_transition_probabilities(s, x, F, a)\n",
    "\n",
    "    return (transition, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_mdp():\n",
    "    \"\"\"Solve the problem as a finite horizon Markov decision process.\n",
    "    The optimal policy at each stage is found using backwards induction.\n",
    "    Possingham and Tuck report strategies for a 50 year time horizon, so the\n",
    "    number of stages for the finite horizon algorithm is set to 50. There is no\n",
    "    discount factor reported, so we set it to 0.96 rather arbitrarily.\n",
    "    Returns\n",
    "    -------\n",
    "    sdp : mdptoolbox.mdp.FiniteHorizon\n",
    "        The PyMDPtoolbox object that represents a finite horizon MDP. The\n",
    "        optimal policy for each stage is accessed with mdp.policy, which is a\n",
    "        numpy array with 50 columns (one for each stage).\n",
    "    \"\"\"\n",
    "    transition, reward = get_transition_and_reward_arrays(0.5)\n",
    "    sdp = mdp.FiniteHorizon(transition, reward, 0.96, 50)\n",
    "    sdp.run()\n",
    "    return sdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    \"\"\"Print out a policy vector as a table to console\n",
    "    Let ``S`` = number of states.\n",
    "    The output is a table that has the population class as rows, and the years\n",
    "    since a fire as the columns. The items in the table are the optimal action\n",
    "    for that population class and years since fire combination.\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : array\n",
    "        ``p`` is a numpy array of length ``S``.\n",
    "    \"\"\"\n",
    "    p = np.array(policy).reshape(POPULATION_CLASSES, FIRE_CLASSES)\n",
    "    print(\"    \" + \" \".join(\"%2d\" % f for f in range(FIRE_CLASSES)))\n",
    "    print(\"    \" + \"---\" * FIRE_CLASSES)\n",
    "    for x in range(POPULATION_CLASSES):\n",
    "        print(\" %2d|\" % x + \" \".join(\"%2d\" % p[x, f] for f in\n",
    "                                     range(FIRE_CLASSES)))\n",
    "\n",
    "def simulate_transition(s, x, F, a):\n",
    "    \"\"\"Simulate a state transition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : float\n",
    "        The class-independent probability of the population staying in its\n",
    "        current population abundance class.\n",
    "    x : int\n",
    "        The population abundance class of the threatened species.\n",
    "    F : int\n",
    "        The time in years since last fire.\n",
    "    a : int\n",
    "        The action undertaken.\n",
    "    Returns\n",
    "    -------\n",
    "    x, F : int, int\n",
    "        The new abundance class, x, of the threatened species and the new years\n",
    "        last fire class, F.\n",
    "    \"\"\"\n",
    "    check_probability(s)\n",
    "    check_population_class(x)\n",
    "    check_fire_class(F)\n",
    "    check_action(a)\n",
    "\n",
    "    r = get_habitat_suitability(F)\n",
    "    F = transition_fire_state(F, a)\n",
    "\n",
    "    if x == POPULATION_CLASSES - 1:\n",
    "        # pass with probability 1 - (1 - s)*(1 - r)\n",
    "        if np.random.random() < (1 - s)*(1 - r):\n",
    "            x -= 1\n",
    "    elif 0 < x < POPULATION_CLASSES - 1:\n",
    "        # pass with probability s\n",
    "        if np.random.random() < 1 - s:\n",
    "            if np.random.random() < r: # with probability (1 - s)r\n",
    "                x += 1\n",
    "            else: # with probability (1 - s)(1 - r)\n",
    "                x -= 1\n",
    "\n",
    "    # Add the effect of a fire, making sure x doesn't go to -1\n",
    "    if a == ACTION_BURN and (x > 0):\n",
    "        x -= 1\n",
    "\n",
    "    return x, F\n",
    "\n",
    "def _run_tests():\n",
    "    \"\"\"Run tests on the modules functions.\n",
    "    \"\"\"\n",
    "    assert get_habitat_suitability(0) == 0\n",
    "    assert get_habitat_suitability(2) == 0.4\n",
    "    assert get_habitat_suitability(5) == 1\n",
    "    assert get_habitat_suitability(8) == 0.7\n",
    "    assert get_habitat_suitability(10) == 0.5\n",
    "    assert get_habitat_suitability(15) == 0.5\n",
    "    state = convert_index_to_state(STATES - 1)\n",
    "    assert state == (POPULATION_CLASSES - 1, FIRE_CLASSES - 1)\n",
    "    state = convert_index_to_state(STATES - 2)\n",
    "    assert state == (POPULATION_CLASSES -1, FIRE_CLASSES - 2)\n",
    "    assert convert_index_to_state(0) == (0, 0)\n",
    "    for idx in range(STATES):\n",
    "        state1, state2 = convert_index_to_state(idx)\n",
    "        assert convert_state_to_index(state1, state2) == idx\n",
    "    print(\"Tests complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SDP = solve_mdp()\n",
    "print_policy(SDP.policy[:, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
